# ğŸ§  AI-Image-Descriptor
![Python](https://img.shields.io/badge/Python-3.12-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![HuggingFace-Space](https://img.shields.io/badge/Live-HuggingFace-yellow)
![Gradio](https://img.shields.io/badge/Built%20with-Gradio-orange)

A simple yet powerful web app that uses the BLIP model to generate intelligent captions for images â€” either uploaded by the user or scraped from any URL. Built with Gradio, Transformers, and Torch.

ğŸ§ª Live Demo

ğŸ‘‰ Try it here: https://huggingface.co/spaces/layer-rep/Image_Captions

### ğŸ–¼ï¸ Screenshot Preview 
![image](https://github.com/user-attachments/assets/6b1087d5-1b4a-4b90-9d7c-4d5c49bb4f97)



> âœ… Works locally with Python 3.12  
> âœ… Compatible with CPU and GPU  
> âœ… Lightweight and fast â€” uses `Salesforce/blip-image-captioning-base`  
> âœ… Built-in UI using Gradio  

---

## ğŸš€ Features

- **Upload images** or paste a **webpage URL**
- Generates **descriptive captions** using BLIP
- Captions the **first 3 usable images** on any webpage
- Fast setup with a **single Python file**
- One-click **copy-to-clipboard**
- Clean, responsive **Gradio UI**
- Fully documented, beginner-friendly code

---

## ğŸ–¼ï¸ Demo Preview

> ğŸ“¸ *(Optional: You can add a screenshot here using `![Alt Text](url)`)*  
*Drag and drop images, or caption directly from a URL.*

---

## ğŸ§  Model Used

- [`Salesforce/blip-image-captioning-base`](https://huggingface.co/Salesforce/blip-image-captioning-base)  
  A lightweight BLIP model optimized for fast, local captioning without tokenizer issues.

---

## ğŸ“¦ Requirements

All dependencies are listed in `requirements.txt`. Install with:

```bash
pip install -r requirements.txt


ğŸ› ï¸ Setup Instructions
âœ… Python 3.12 compatible

1. Clone the repo
git clone https://github.com/srimaansri/AI-Image-Descriptor.git
cd AI-Image-Descriptor

2. (Optional) Create a virtual environment

python -m venv venv
venv\Scripts\activate  # On Windows

3. Install dependencies
pip install -r requirements.txt

4. Run the app
python gradio_blip2_app.py

The app will be live at:
ğŸ“ http://127.0.0.1:7860

ğŸ’¡ How It Works
The app uses BLIP to encode images and generate natural-language captions.

In image mode: Upload an image â†’ get caption.

In URL mode:

Extracts <img> tags using BeautifulSoup

Filters and downloads valid images

Runs captioning on the first 3 usable images

All wrapped in a clean, responsive Gradio interface.

ğŸ“ File Structure

AI-Image-Descriptor/
â”œâ”€â”€ gradio_blip2_app.py                 # Main app script
â”œâ”€â”€ requirements.txt                    # All dependencies
â”œâ”€â”€ Screenshot 2025-05-07 231113.png    # UI preview image
â”œâ”€â”€ .gitignore                          # Clean repo: ignores venv, __pycache__, etc.

ğŸ“¢ License
This project is licensed under the MIT License.


âœ¨ Author
Made with â¤ï¸ by srimaansri


